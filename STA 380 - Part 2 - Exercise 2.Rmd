---
title: 'STA 380 Part2 : Excercise 2'
author: "Akhilesh Reddy Narapareddy"
date: "August 17, 2018"
output: 
  md_document:
  variant: markdown_github

---


### Flights at ABIA

#### Objective of this analysis is to identify the general trends in the flight delay times and suggest the right time/month to the passenger depending on the destination that he/she wants to fly to

1.We will first assess the flight volumes across months and time of the day  
2.Identify the general delay trends during those time frames  
3.Identify the top destinations by volume of flights    
4.Identify the flight volumes to those destinations    
5.Recommend the month and time of the day to travel to these destinations based on the historical median delays of those flights  

#### 1. Overall trend  
```{r, echo = FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
library(dplyr)
library(gridExtra)
setwd("E:\\Summer II\\Intro to Predictive Modelling - Part 2\\Exercise 2\\")
air_data = read.csv("ABIA.csv",na.strings= c('',NA))
# summary(air_data)

```

Understanding the magnitude of flights that are coming into Austin will help us in assessing the traffic of Austin airport during different seasons.  
In addition to that,percentage of fligths that are delayed during those months would give us an insight into the operating logistics of Austin airport  

```{r, echo = FALSE, message=FALSE, warning=FALSE }
# Total scheduled flights by month during 2008 from AUS
air_data$Month = as.factor(air_data$Month)

#Remove cancelled flights
air_data1 = subset(air_data,air_data$Cancelled == 0)
air_data1$time = as.integer(air_data1$CRSDepTime/100)
air_data1$time  = as.factor(air_data1$time )

air_data1$time_period <- NA
air_data1$time_period[air_data1$CRSDepTime <= 600] <- "Early Morning"
air_data1$time_period[air_data1$CRSDepTime > 600 & air_data1$CRSDepTime <= 900] <- "Morning"
air_data1$time_period[air_data1$CRSDepTime > 900 & air_data1$CRSDepTime <= 1200] <- "Pre-Afternoon"
air_data1$time_period[air_data1$CRSDepTime > 1200 & air_data1$CRSDepTime <= 1500] <- "Afternoon"
air_data1$time_period[air_data1$CRSDepTime > 1500 & air_data1$CRSDepTime <= 1800] <- "Evening"
air_data1$time_period[air_data1$CRSDepTime > 1800 & air_data1$CRSDepTime <= 2100] <- "Night"
air_data1$time_period[air_data1$CRSDepTime > 2100 & air_data1$CRSDepTime <= 2359] <- "Late Night"


air_data1$time_period <- factor(air_data1$time_period,                                   levels = c("Early Morning", "Morning", "Pre-Afternoon","Afternoon", "Evening", "Night", "Late Night"),ordered = TRUE)

from_AUS = subset(air_data1,air_data1$Origin == 'AUS')
to_AUS = subset(air_data1,air_data1$Dest == 'AUS')

from_AUS$fly_flag = 'from AUS'
to_AUS$fly_flag = 'to AUS'

from_aus_agg <- dplyr :: summarise(group_by(from_AUS,Month,fly_flag),count =n())
to_aus_agg <- dplyr :: summarise(group_by(to_AUS,Month,fly_flag),count =n())

final_vol_data <- rbind(from_aus_agg,to_aus_agg)

flight_vol <- ggplot(from_aus_agg, aes(x = Month, y = count, group = 1)) +
        geom_line() + theme_classic() + ylim(c(0,5000)) + 
        labs(x = "Month", y = "Number of flights",title = "Trend in flight volume",
        subtitle = "All buildings") + theme(
        axis.text.x = element_text(face="bold",color="black", size=8, angle=0),
        axis.text.y = element_text(face="bold", color="black", size=8, angle=0),
        plot.title = element_text(hjust = 0.5),plot.subtitle = element_text(hjust = 0.5)
       )
flight_vol
```

* Number of flights from Austin tend to decline towards the end of the year  
* This might be due to the onset of the holiday season in the last quarter  

#### 2.Distribution of flights that got delayed each month  
```{r, echo = FALSE, message=FALSE, warning=FALSE }

# from_aus_agg_delay = summarise(group_by(from_AUS,Month,dep_delay_flag),count =n())
# from_aus_agg_delay = na.omit(from_aus_agg_delay)

# Total flights vs delayed flights from Austin
from_AUS$dep_delay_flag = ifelse(from_AUS$DepDelay > 0,1,0)
from_AUS$dep_delay_flag = as.factor(from_AUS$dep_delay_flag)

from_aus_agg_delay = from_AUS %>%
  group_by(Month,dep_delay_flag) %>%
  dplyr :: summarise (count = n())

from_aus_agg_delay_total = from_AUS %>%
  group_by(Month) %>%
  dplyr :: summarise (count = n())

from_aus_month_final = merge(from_aus_agg_delay ,from_aus_agg_delay_total, by = 'Month')
from_aus_month_final$freq = from_aus_month_final$count.x/from_aus_month_final$count.y

p4 <- ggplot() + theme_bw() + 
  geom_bar(aes(y = freq, x = Month, fill = dep_delay_flag ), data = from_aus_month_final, stat="identity") + geom_text(data=from_aus_month_final, aes(x = Month, y =  freq,label=paste0(sprintf("%.1f",(1-freq)*100),"%")),position = position_stack(vjust = 0.2), size=4,fontface='bold', colour = 'white') +
  theme(legend.position="bottom", legend.direction="horizontal",
        legend.title = element_blank()) + scale_y_continuous(labels = scales::percent) +
         labs(x="Month", y="Percentage") +
  ggtitle("Trend in delays (%)")
p4
# Work on labels
```

* Delays in departure are relatively less during the Sep,Oct,Nov  
* March,June and December encounter delays for maximum number of flights  

#### 3. Time of the day  
Let's look at the analysis with the location  
* Top destinations from Austin
Time of the day has been grouped into different types for the convenience of analysis  
Number of flights from Austin decrease towards the end of the year by about 25%

It is important to continue further to look at the pattern during hour of the day, to understand the average traffic during a particular time of the day  
```{r, echo = FALSE, message=FALSE, warning=FALSE }

# Removing the NA from time period
from_AUS_time = subset(from_AUS,from_AUS$time_period != 'NA')
# Calculate the average number of flights during a particular hour of the day
from_aus_agg_hour <- dplyr :: summarise(group_by(from_AUS_time,time_period),count =round(n()/365))
flight_vol_hour <- ggplot(from_aus_agg_hour, aes(x = time_period, y = count, group = 1)) +
        geom_bar(stat = 'identity',fill = 'darkolivegreen',width = 0.4) + theme_classic() +
        labs(x = "Time of the Day", y = "Number of flights",title = "Trend in flight volume",subtitle = "On an average across the year") + theme(
        axis.text.x = element_text(face="bold",color="black", size=8, angle=0),
        axis.text.y = element_text(face="bold", color="black", size=8, angle=0),
        plot.title = element_text(hjust = 0.5),plot.subtitle = element_text(hjust = 0.5)
       )
flight_vol_hour
```

* We can observe maximum number of flights during Morning(6 A.M to 9 A.M) and Evening(between 3 P.M to 6 P.M)  
* Also there is a steady decline towards the end of the day  

#### Distribution of delays for the scheduled flights during the respective times  

```{r, echo = FALSE, message=FALSE, warning=FALSE }
# Calculating the distribution of delayed flights during hour of the day

from_aus_agg_delay_hour = from_AUS_time %>%
  group_by(time_period,dep_delay_flag) %>%
  dplyr :: summarise (count = n())

from_aus_agg_delay_total_hour = from_AUS_time %>%
  group_by(time_period) %>%
  dplyr :: summarise (count = n())

from_aus_month_final_hour = merge(from_aus_agg_delay_hour ,from_aus_agg_delay_total_hour, by = 'time_period')
from_aus_month_final_hour$freq = from_aus_month_final_hour$count.x/from_aus_month_final_hour$count.y

flights_hour_delay <- ggplot() + theme_bw() + 
  geom_bar(aes(y = freq, x = time_period, fill = dep_delay_flag ), data = from_aus_month_final_hour, stat="identity") + geom_text(data=from_aus_month_final_hour, aes(x = time_period, y =  freq,label=paste0(sprintf("%.1f",(1-freq)*100),"%")),position = position_stack(vjust = 0.1), size=4,fontface='bold', colour = 'white') + scale_fill_manual(values = c('darkolivegreen','red4')) + theme(legend.position="bottom", legend.direction="horizontal",legend.title = element_blank()) + scale_y_continuous(labels = scales::percent) +
  labs(x="Hour", y="Percentage") +
  ggtitle("Trend in delays by hour (%)")
flights_hour_delay
```

* Number of flight delays increase towards the end of the day  

#### 4. Identifying the top destinations by flight volume  and their flight volumes across months  
```{r, echo = FALSE, message=FALSE, warning=FALSE }
from_aus_agg_dest <- dplyr :: summarise(group_by(from_AUS,Month,Dest),count =n())

#Top destinations by total volume
dest = dplyr :: summarise(group_by(from_AUS,Dest),count =n())
top_5_dest = head(arrange(dest, desc(count)),5)[,1]

# Flight volume for top 10 destinations across months
dest_5 = merge(from_aus_agg_dest,top_5_dest,by = 'Dest' )

flight_vol_dest <- ggplot(dest_5, aes(x = Month, y = count, group = Dest)) +
        geom_line(aes(color = Dest)) + theme_classic() + ylim(c(0,800)) + 
        labs(x = "Month", y = "Number of flights",title = "Trend in flight volume",
        subtitle = "Across months") + theme(
        axis.text.x = element_text(face="bold",color="black", size=8, angle=0),
        axis.text.y = element_text(face="bold", color="black", size=8, angle=0),
        plot.title = element_text(hjust = 0.5),plot.subtitle = element_text(hjust = 0.5)
       )
flight_vol_dest

```

* Although, DAL started with a higher volume at the start of the year, there was a sudden dip during July  
* Apart from that, all other destinations have a steady trend across months  
* There is small bump during october in flights for all destinations  

#### 5.Identification of month and time of the day with maximum delays for the top 5 destinations  
##### Month  
Lets see the median delay that we can expect if we are planning to fly to those destinations during different hours of the day  

```{r, echo = FALSE, message=FALSE, warning=FALSE }
from_AUS_delay = subset(from_AUS,from_AUS$DepDelay > 0)
# Top 5 destinations
from_AUS_delay_dest = merge(from_AUS_delay,top_5_dest,by = 'Dest')

from_aus_agg_dest_delay = from_AUS_delay_dest %>%
  group_by(Month,Dest) %>%
  dplyr :: summarise (med_delay = median(DepDelay))

ggplot(from_aus_agg_dest_delay, aes(Month, Dest)) + geom_tile(aes(fill = med_delay),
     colour = "white") + scale_fill_gradient(low = "white",high = "red") + labs(x = "Month", y = "Median delay in minutes",title = "Trend in flight delay",
        subtitle = "Across months") + theme(
        axis.text.x = element_text(face="bold",color="black", size=8, angle=0),
        axis.text.y = element_text(face="bold", color="black", size=8, angle=0),
        plot.title = element_text(hjust = 0.5),plot.subtitle = element_text(hjust = 0.5)
       )
  
```

* Based on the heat map,we can infer that flight to Denver in Feb and to DFW and IAH in August face a median delay of 16 mins which is the highest among the top 5 destinations  
* In addition to that, flights to Dallas are almost prompt during July to October    

##### Time of the day    
```{r, echo = FALSE, message=FALSE, warning=FALSE }

from_aus_agg_dest_delay_time = from_AUS_delay_dest %>%
  group_by(time_period,Dest) %>%
  dplyr :: summarise (med_delay = median(DepDelay))

ggplot(from_aus_agg_dest_delay_time, aes(time_period, Dest)) + geom_tile(aes(fill = med_delay), colour = "white") + scale_fill_gradient(low = "white",high = "red") + theme_classic() + labs(x = "Time of the day", y = "Median delay in minutes",title = "Trend in flight delay") + theme(
        axis.text.x = element_text(face="bold",color="black", size=8, angle=0),
        axis.text.y = element_text(face="bold", color="black", size=8, angle=0),
        plot.title = element_text(hjust = 0.5),plot.subtitle = element_text(hjust = 0.5)
       )

```

**Recommendations:**  

* We can observe that ther are no early morning flights to DAL and DEN and no Late night flights to any destination except DAL  
* If you are travelling to Pheonix, we can  except maximum delays at Early morning and night    
* While travelling to Denver,it will be good if we avoid flights during pre-afternoon and evening as they tend to get delayed  
* Morning is the best time to go to Dallas    

### Author attribution  
**Objective: To attribute the article in a document to the respective author using classification models and text analytics**  

Let us consider a step by step model to create a classifiation model from the text data to identify the author of the article  

1.Read the data in suitable formats required for the exercise  
2.Perform all the pre-processing tasks on the dataset extracted like removing stop words, changing to lower case e.t.c  
3.Create the TF-IDF matrix for both train and test data  
4.Dimensionality reduction  
5.Use the TF-IDF data to create models and compare the accuracy across models to identify the author of a particular article  

**1.Read the data in suitable formats**  
As the data is a text file, we will have to convert into a format that is acceptable for performing the pre-processing functions.After importing the files, a corpus has been created with all the documents separately for train and test documents

```{r, message=FALSE, warning=FALSE}
library(tm)
library(proxy)
library(randomForest)
library(kknn)
library(dplyr)
library(caret)

# read in train data and create DTM
author_names_train <- dir("./ReutersC50/C50train")

file_list_train <- NULL
class_labels_train <- NULL

for (name in author_names_train){
  file_list_train <- c(file_list_train, Sys.glob(paste0('./ReutersC50/C50train/', name,'/*.txt')))
  class_labels_train <- c(class_labels_train, rep(name, each = length(Sys.glob(paste0('./ReutersC50/C50train/', name,'/*.txt')))))
}

# define the function that will read in the files
readerPlain = function(fname){
  readPlain(elem = list(content = readLines(fname)), 
            id = fname, language = 'en') }

# read in the files and store them as a list
all_files_train <- lapply(file_list_train, readerPlain)

# give each file a representative name

file_names_train <- file_list_train %>%
  strsplit("/") %>%
  lapply(tail,n = 2) %>%
  lapply(paste0, collapse = "") %>%
  unlist

# create a dataframe with doc_id as author-article and text as the text in that article
text_vector_train <- NULL

for(i in 1:length(file_names_train)){
  text_vector_train <- c(text_vector_train, paste0(content(all_files_train[[i]]), collapse = " "))
}

# dataframe with text and document_id
text_df_train <- data.frame(doc_id = file_names_train,
                            text = text_vector_train)

# convert the dataframe to a Corpus
train_corpus_raw <- VCorpus(DataframeSource(text_df_train))


# read in the test documents
author_names_test <- dir("./ReutersC50/C50test")

file_list_test <- NULL
class_labels_test <- NULL

for (name in author_names_test){
  file_list_test <- c(file_list_test, Sys.glob(paste0('./ReutersC50/C50test/', name,'/*.txt')))
  class_labels_test <- c(class_labels_test, rep(name, each = length(Sys.glob(paste0('./ReutersC50/C50test/', name,'/*.txt')))))
}

# read in the files and store them as a list
all_files_test <- lapply(file_list_test, readerPlain)

# give each file a representative name

file_names_test <- file_list_test %>%
  strsplit("/") %>%
  lapply(tail,n = 2) %>%
  lapply(paste0, collapse = "") %>%
  unlist

# create a dataframe with doc_id as author-article and text as the text in that article
text_vector_test <- NULL

for(i in 1:length(file_names_test)){
  text_vector_test <- c(text_vector_test, paste0(content(all_files_test[[i]]), collapse = " "))
}


# dataframe with text and document_id
text_df_test <- data.frame(doc_id = file_names_test,
                           text = text_vector_test)

# convert the dataframe to a Corpus
test_corpus_raw <- VCorpus(DataframeSource(text_df_test))

```
**2.Pre-Processing both train and test data**  

While dealing with text data, it is optimal to ignore numbers,punctuations,white spaces as they don't help much in gaining an insight into the patterns present in the text.It is also important to ignore the **Stop words**(words like is/an/the e.t.c) as they occur multiple times with no real information being added to the models.In this step, we have removed the stop words, punctuations, numbers to proceed with the analysis    
```{r, message=FALSE, warning=FALSE}

#train data
# pre-processing to remove punctuations, spaces, etc.
train_corpus_preproc <- train_corpus_raw
train_corpus_preproc <- tm_map(train_corpus_preproc, content_transformer(tolower))
train_corpus_preproc <- tm_map(train_corpus_preproc, content_transformer(removeNumbers)) # remove numbers
train_corpus_preproc <- tm_map(train_corpus_preproc, content_transformer(removePunctuation)) # remove punctuation
train_corpus_preproc <- tm_map(train_corpus_preproc, content_transformer(stripWhitespace)) ## remove excess white-space
train_corpus_preproc <- tm_map(train_corpus_preproc, content_transformer(removeWords), stopwords("en")) # remove stop words


#test data

# pre-processing to remove punctuations, spaces, etc.
test_corpus_preproc <- test_corpus_raw
test_corpus_preproc <- tm_map(test_corpus_preproc, content_transformer(tolower)) # make everything lowercase
test_corpus_preproc <- tm_map(test_corpus_preproc, content_transformer(removeNumbers)) # remove numbers
test_corpus_preproc <- tm_map(test_corpus_preproc, content_transformer(removePunctuation)) # remove punctuation
test_corpus_preproc <- tm_map(test_corpus_preproc, content_transformer(stripWhitespace)) ## remove excess white-space
test_corpus_preproc <- tm_map(test_corpus_preproc, content_transformer(removeWords), stopwords("en")) # remove stop words

```
**3.Create TF-IDF matrix for both train and test data**  

Next step in this process is to create a TF-IDF matrix of the corpus of documents that we have created. TF-IDF is a combination of TF(Term frequency) and IDF(Inverse document frequency). TF gives the number of times a word occurs in a document while IDF gives less weightage to words that occur in multiple documents and is not useful in identifying the style of anyone single author.  

After the creation of the TF-IDF matrix, sparsing is one more step that is recommended while dealing with text data. **In sparsing, we remove terms that occur less frequently among all the documents**.This is generally decided by a threshold that is set heuristically based on the TF-IDF matrix. In this analysis, the threshold is set at **99%(words that are not present in 99% of the documents will be removed from the analysis)**      

```{r, message=FALSE, warning=FALSE}
#train
# convert the corpus to a document term matrix
DTM_train <- DocumentTermMatrix(train_corpus_preproc)

# remove sparse terms from the DTM_train
DTM_train <- removeSparseTerms(DTM_train, 0.99)


#test
# convert the corpus to a document term matrix
DTM_test <- DocumentTermMatrix(test_corpus_preproc, 
                               control = list(dictionary = Terms(DTM_train)))

# calculate the TF-IDF for each term in the DTM
tfidf_train <- weightTfIdf(DTM_train)
tfidf_test <- weightTfIdf(DTM_test)

X_train <- as.matrix(tfidf_train)
X_test <- as.matrix(tfidf_test)

```

**4.Dimensionality reduction**   

Often while dealing with text data, we encounter with dimensionality problem. Due to the sheer volume of words present in any language, we often end up with thousands of words(columns) in the TF-IDF matrix. This makes it comptutationally heavy for any system to perform any modelling on the data. Dimensionality is the go to solution in these situations.  
For this problem, i have used **PCA(Prinipal Component Analysis)** to reduce the number of dimensions in the data set.  
After running PCA, a total of 350 components have been selected for further analysis as they explain about 50% of the total variaion in the data.

There is one interseting step that we perform during PCA. As we have 2 different datasets(train and test) to deal with, it is important to ensure that both the datasets have similar type of principal components that are aligned towards the same subspace respectively.  

**To achieve that, we will use the loadings, rotations and other attributes from the PC object of the train data to align the components of the test data in a similar orientation as the train data.**    

```{r, message=FALSE, warning=FALSE}

pca_train = prcomp(X_train, scale=TRUE)
# plot(pca_train)

# we will take the first 350 components because they explain 50% of the variance in the data
# summary(pca_train)$importance[3,]

X_train <- pca_train$x[,1:350]
X_train <- cbind(X_train, class_labels_train)
loading_train <- pca_train$rotation[,1:350]

# multiply to get a test matrix with the principal component values
X_test_pc <- scale(X_test) %*% loading_train
X_test_pc <- as.data.frame(X_test_pc)

rm(list = c("all_files_test", "all_files_train", "test_corpus_preproc", "train_corpus_preproc", "text_df_test", "text_df_train",
            "author_names_test", "author_names_train", "file_list_test", "file_list_train", "i", "name", "text_vector_test",
            "text_vector_train", "file_names_test", "file_names_train", "DTM_test", "DTM_train", "pca_train",
            "test_corpus_raw", "train_corpus_raw", "tfidf_train", "tfidf_test"))

X_train <- as.data.frame(X_train)

for (name in names(X_train)){
  if (name == "class_labels_train"){
    next
  }else{
    X_train[[name]] <- as.numeric(as.character(X_train[[name]]))
  }
}

X_train$class_labels_train <- as.factor(X_train$class_labels_train)
# 
# plot(summary(pca_train)$importance[3,], main = "PCA Analysis Train", xlab = "Components",
#      ylab = "Cumulative % Variance Explained")
```

**5. Models using features from PCA and identifying the authors**  

**Attribution Model 1: knn**  

It makes sense that documents closer to each other (using similar terms) in terms of the Manhattan distance would be from the same author. Lets try K-Nearest Neighbors to predict the author for each document in the test set!  

1.We will use a K Nearest neighbor model and look for the best K-value in the set {5,7,9,11}.  
2.For the distance metric, we will use the Manhattan distance!  

```{r, message=FALSE, warning=FALSE}

# knn model - 29% max accuracy at k = 9
library(kknn)

# a vector to store the accuracies of the knn model
accuracies <- NULL

# try knn with 5,7,9 and 11 nearest neighbors
for (i in c(5,7,9,11)){
  knn_model <- kknn(class_labels_train ~ .,
                    X_train,
                    X_test_pc,
                    distance = 1,
                    k= i,
                    kernel = 'rectangular')
  
  accuracies <- c(accuracies,sum(knn_model$fitted.values == class_labels_test)/length(class_labels_test))
}

plot(c(5,7,9,11), accuracies, main = "KNN accuracy vs K", xlab = "K-Values", ylab = "Accuracy Score", lty = 1)
```
With Knn, we get an overall accuracy of ~35%. Let's see how the model worked for different authors  

Following are the top authors that the model gets right!  

```{r,echo = FALSE}
#knn

knn_prediction_vs_actual <- as.data.frame(table(knn_model$fitted.values, class_labels_test))
knn_prediction_vs_actual <- knn_prediction_vs_actual %>% filter(Freq > 0)

names(knn_prediction_vs_actual) <- c("Author_Predicted", "Actual_Author", "Frequency")

knn_prediction_vs_actual %>%
  filter(Author_Predicted == Actual_Author) %>%
  group_by(Actual_Author) %>% 
  summarise("Accuracy" = sum(Frequency)/50) %>%
  arrange(-Accuracy) %>%
  head(5)
```

Following are the authors that the model performs very badly  

```{r,echo = FALSE}
knn_prediction_vs_actual %>%
  filter(Author_Predicted == Actual_Author) %>%
  group_by(Actual_Author) %>% 
  summarise("Accuracy" = sum(Frequency)/50) %>%
  arrange(Accuracy) %>%
  head(5)
```

**Attribution model 2: Random Forest**   
Let's run a random forest to check the accuracy of the predictions for author attribution  
```{r, message = FALSE,warning=FALSE}
# Random Forest
rf_model <- randomForest(class_labels_train ~ .,
                         data = X_train,
                         ntree = 1000)
author_predict <- predict(rf_model, X_test_pc, type = "response")
answer <- as.data.frame(table(author_predict, class_labels_test))
answer$correct <- ifelse(answer$author_predict==answer$class_labels_test, 1, 0)

answer_rf = answer %>% group_by(correct) %>% summarise("Correct" = sum(Freq))

rf_accuracy <- sum(answer$Freq[answer$correct==1])*100/sum(answer$Freq)
  
print(paste0("Accuracy is ", rf_accuracy))
```
* Accuracy of Random Forest is `r rf_accuracy` which is better than knn  

Lets see what authors have been attributed with higher accuracy  
```{r,echo = FALSE}
names(answer) <- c("Author_Predicted", "Actual_Author", "Freq", "Correct")

answer %>%
  filter(Author_Predicted == Actual_Author) %>%
  group_by(Actual_Author) %>% 
  summarise("Accuracy" = sum(Freq)/50) %>%
  arrange(-Accuracy) %>%
  head(5)

```
* Authors that the model predicted with minimum accuracy  
```{r,echo = FALSE}
answer %>%
  filter(Author_Predicted == Actual_Author) %>%
  group_by(Actual_Author) %>% 
  summarise("Accuracy" = sum(Freq)/50) %>%
  arrange(Accuracy) %>%
  head(5)

```
**Attribution model 3:**
**XGBoost**  
```{r,message=FALSE,warning=FALSE}
# XGBoost model
library(xgboost)
train_data_xgboost_matrix <- data.matrix(X_train[,1:350])
test_data_xgboost_matrix <- data.matrix(X_test_pc)
class_labels_train = data.matrix(class_labels_train)

dtrain <- xgb.DMatrix(data = train_data_xgboost_matrix, label = as.numeric(X_train[,351])-1,
                      missing = NA)
dtest <- xgb.DMatrix(data = test_data_xgboost_matrix, label = as.numeric(as.factor(class_labels_test)) - 1)

boost_model <- xgboost(data = dtrain,
                       nround = 30, # max number of boosting iterations
                       # distribution = "multinomial",
                       objective = "multi:softmax",
                       eta = 0.1,
                       num_class = 50,
                       max_depth = 3
                       )

author_predict <- predict(boost_model, dtest)
XGbacc <- mean(author_predict == (as.numeric(as.factor(class_labels_test)) - 1))*100
print(paste0("Accuracy is :",XGbacc))
```
We get `r XGbacc` accuracy from XGBoost which is not better than the Random forest model  

**Conclusion:**  

Text analytics is often computationally heavy especially when dealing with thousands of documents. Dimensionality reduction is the best way to go further with the analysis.There is also a downside to this as removing the terms from the documents might result in reduced accuracies as we have seen earlier. But that is trade-off that is essential while dealing with data of this scale.  
Random forest gave us the best accuracy among all the other models with a value of about ~60%. This can be further improved if we can do an ensemble of the above models that are being used.  


## Association Rule mining  

Association rule mining is a very interesting and important topic in retail analytics.Being able to find the associations between various products or services is the first step towards providing personalized recommendations.  
Apriori is one of the famous algorithms that is generally used for creating association rules and is also used for creating association rules for this problem.      

Steps followed to create the association rules  

1.Data pre processing  
2.Running the apriori algorithm  
3.Visualization of the Output  

Let's create the rules that will help in identifying the associations     

```{r,echo = FALSE,message = FALSE,warning = FALSE}
library('R.utils')
library(arules)
library(arulesViz)
library(grid)
## File
file <- "groceries.txt"

temp = read.delim2(file, header = FALSE, sep = "\t", dec = ",")
n = dim(temp)[1]

temp$lists = strsplit(as.character(temp$V1),",")
temp <- tibble::rowid_to_column(temp, "ID")

df = data.frame(ID=integer(),
                items=character())

for (i in 1:n) {
k = data.frame(ID=i, y=temp[i,3])
names(k) = c('ID','items')
df = rbind(df,k)
}

# Turn ID into a factor
df$user = factor(df$ID)
items = split(x=df$items, f=df$ID)

## Remove duplicates ("de-dupe")
items = lapply(items, unique)

## Cast this variable as a special arules "transactions" class.
itemtrans = as(items,"transactions")

# Now run the 'apriori' algorithm
# Look at rules with support > .005 & confidence >.1 & length (# artists) <= 5
itemrules = apriori(itemtrans, 
                     parameter=list(support=.001, confidence=.4, maxlen=10))

inspect(subset(itemrules, subset=lift > 10 & confidence > 0.5))

sub1 = subset(itemrules, subset=confidence > 0.4 & support > 0.001)

plot(head(sub1, 10, by='lift'), method='graph')

```

**Association rules**  

1.There is a high probability for people to red/blush wine if they buy bottled beer and liquor  
2.Buying soda and popcorn has a high association with salty snack  
3.Processed cheese and white bread has a high association with buying ham based on the lift 
4.Some rules are obvious like buying flour and baking powder has a high association with sugar  

**END** 